{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "pad_id = 4002\n",
    "weight = [1 for i in range(4004)]\n",
    "weight[4002] = 0\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, en_dims, en_voc, zh_dims, zh_voc, dropout, en_hiddens, zh_hiddens):\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.weight = torch.Tensor(weight)\n",
    "        self.en_dims = en_dims\n",
    "        self.zh_dims = zh_dims\n",
    "        self.en_hiddens = en_hiddens\n",
    "        self.zh_hiddens = zh_hiddens\n",
    "\n",
    "        self.en_embedding = torch.nn.Embedding(num_embeddings=en_voc, embedding_dim=en_dims)\n",
    "        self.zh_embedding = torch.nn.Embedding(num_embeddings=zh_voc, embedding_dim=zh_dims)\n",
    "\n",
    "        self.enc_fw_lstm = torch.nn.LSTM(input_size=self.en_dims,\n",
    "                                   hidden_size=self.en_hiddens, \n",
    "                                   dropout=dropout,\n",
    "                                   batch_first=False)\n",
    "        \n",
    "        self.enc_bw_lstm = torch.nn.LSTM(input_size=self.en_dims,\n",
    "                                   hidden_size=self.en_hiddens, \n",
    "                                   dropout=dropout,\n",
    "                                   batch_first=False)\n",
    "        \n",
    "        self.dec_lstm_cell = torch.nn.LSTMCell(input_size=self.en_hiddens*2+self.zh_dims,\n",
    "                                               hidden_size=self.zh_hiddens, \n",
    "                                               bias=True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=self.zh_hiddens, out_features=zh_voc)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.cost_func = nn.CrossEntropyLoss(weight=self.weight)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, inputs, gtruths, is_train):\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(inputs).long()).cuda()\n",
    "        gtruths = Variable(torch.from_numpy(gtruths)).long().cuda()\n",
    "        \n",
    "        inputs = self.en_embedding(inputs)\n",
    "        gtruths = self.zh_embedding(gtruths)\n",
    "        \n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "        gtruths = torch.transpose(gtruths, 0, 1)\n",
    "        \n",
    "        enc_fw, _ = self.enc_fw_lstm(inputs)\n",
    "        enc_bw, _ = self.enc_bw_lstm(inputs)\n",
    "        \n",
    "        #print ('enc_fw size: ', enc_fw[-1, :, :].size(), ';  enc_bw size: ', enc_bw[0, :, :].size())\n",
    "        encoder = torch.cat([enc_fw[-1, :, :], enc_bw[0, :, :]], -1)\n",
    "    \n",
    "        #print ('encoder.size(): ', encoder.size())\n",
    "        \n",
    "        #forward lstm\n",
    "        hx = Variable(torch.randn(encoder.size(0), self.zh_hiddens)).cuda()\n",
    "        cx = Variable(torch.randn(encoder.size(0), self.zh_hiddens)).cuda()\n",
    "        \n",
    "        logits = [0 for i in range(gtruths.size(0))]\n",
    "        predic = [0 for i in range(gtruths.size(0))]\n",
    "        \n",
    "        \n",
    "        for i in range(gtruths.size(0)):\n",
    "            \n",
    "            if is_train:\n",
    "                inp = torch.cat([encoder, gtruths[i]], -1)\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    inp = torch.cat([encoder, gtruths[0]], -1)\n",
    "                else:\n",
    "               \n",
    "                    prev = self.zh_embedding(predic[i-1])\n",
    "                    prev = prev.view(prev.size(1), prev.size(2))\n",
    "                    inp = torch.cat([encoder, prev], -1)\n",
    "                \n",
    "            hx, cx = self.dec_lstm_cell(inp, (hx, cx))\n",
    "            logits[i] = self.fc(hx)\n",
    "\n",
    "            _, predic[i] = torch.max(logits[i], 1)\n",
    "            logits[i] = logits[i].view(1, logits[i].size(0), logits[i].size(1))\n",
    "            \n",
    "            #print (logits[i])\n",
    "            #print (predic[i])\n",
    "            #print (predic[i].size())\n",
    "            predic[i] = predic[i].view(1, predic[i].size(0))\n",
    "            #print (predic[i])\n",
    "        \n",
    "        predic = torch.cat(predic, 0)\n",
    "        predic = torch.transpose(predic, 0, 1)\n",
    "        return torch.cat(logits), predic.data.cpu().numpy()\n",
    "            \n",
    "        \n",
    "    def get_loss(self, logits, labels):\n",
    "        \n",
    "        labels = Variable(torch.from_numpy(labels)).long().cuda()\n",
    "        \n",
    "        labels = torch.transpose(labels, 0, 1)\n",
    "        \n",
    "        #print (labels.size())\n",
    "        #print (logits.size())\n",
    "        \n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.contiguous().view(-1)\n",
    "        \n",
    "        #print (logits.size())\n",
    "        #print (labels.size())\n",
    "        \n",
    "        loss = torch.mean(self.cost_func(logits, labels))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Seq2Seq(en_dims = 256,\n",
    "           en_voc = 50004,\n",
    "           zh_dims = 256,\n",
    "           zh_voc = 4004,\n",
    "           dropout = 0.5,\n",
    "           en_hiddens = 128,\n",
    "           zh_hiddens = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq (\n",
       "  (en_embedding): Embedding(50004, 256)\n",
       "  (zh_embedding): Embedding(4004, 256)\n",
       "  (enc_fw_lstm): LSTM(256, 128, dropout=0.5)\n",
       "  (enc_bw_lstm): LSTM(256, 128, dropout=0.5)\n",
       "  (dec_lstm_cell): LSTMCell(512, 256)\n",
       "  (fc): Linear (256 -> 4004)\n",
       "  (softmax): Softmax ()\n",
       "  (cost_func): CrossEntropyLoss (\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
